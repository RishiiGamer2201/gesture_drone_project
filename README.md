# ğŸš Gesture-Controlled Drone System

A real-time hand gesture recognition system for controlling drones using computer vision and machine learning. Control your drone with simple hand gestures - no controller needed!

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-green.svg)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10+-orange.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.13+-red.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## ğŸ¯ Features

### Core Capabilities
- âœ… **10 Static Gestures** - UP, DOWN, LEFT, RIGHT, FORWARD, BACKWARD, HOVER, LAND, FLIP, ROCK
- âœ… **Dynamic Gestures** - Circle, Swipe (4 directions), Photo Capture, Wave
- âœ… **Two-Hand Controls** - Takeoff, Emergency Stop, Follow Mode
- âœ… **Mode Switching** - Toggle between Static and Dynamic modes
- âœ… **ML Model Support** - KNN, CNN, and ANN models for accurate recognition
- âœ… **Real-time Hand Tracking** - 60+ FPS performance with MediaPipe
- âœ… **AR Overlay** - Hand trajectory visualization and drone telemetry
- âœ… **Robotic HUD** - Advanced hand skeleton display

### Advanced Features
- ğŸ® **Dual Mode System** - Prevent gesture confusion with exclusive modes
- ğŸ¤– **ML Model Integration** - Automatic model detection and loading
- ğŸ“¸ **Photo Capture** - Take photos using hand gestures
- ğŸ‘¤ **Follow Mode** - Drone follows your hand movements
- ğŸ“Š **Live Telemetry** - Real-time position and status display
- ğŸ¨ **Custom HUD** - Robotic-style hand visualization

## ğŸš€ Quick Start

### Prerequisites
```bash
Python 3.8 or higher
Webcam or external camera
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/RishiiGamer2201/gesture_drone_project.git
cd gesture_drone_project
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Run the application**
```bash
python test.py
```

That's it! The system will automatically detect and load any trained models from the `models/` folder.

## ğŸ® Controls

### Two-Hand Gestures (Always Active)
| Gesture | Action | Description |
|---------|--------|-------------|
| âœ‹âœ‹ Two open palms | **TAKEOFF** | Launch the drone |
| âœŠâœŠ Two fists | **EMERGENCY** | Immediate stop and land |
| âœŠâœ‹ Fist + Open | **FOLLOW MODE** | Toggle follow mode |

### Static Mode (Default)
| Gesture | Action | How to Perform |
|---------|--------|----------------|
| â˜ï¸ One finger up | **UP** | Point index finger upward |
| ğŸ‘‡ One finger down | **DOWN** | Point index finger downward |
| ğŸ‘ˆ Point left | **LEFT** | Point index finger left |
| ğŸ‘‰ Point right | **RIGHT** | Point index finger right |
| ğŸ‘ Thumbs up | **FORWARD** | Thumb pointing up |
| ğŸ‘ Thumbs down | **BACKWARD** | Thumb pointing down |
| âœŠ Closed fist | **HOVER** | Make a fist |
| âœ‹ Open palm | **LAND** | Show all 5 fingers |
| âœŒï¸ Peace sign | **FLIP** | Index + middle finger up |
| ğŸ¤˜ Rock sign | **MODE SWITCH** | Index + pinky up |

### Dynamic Mode (After switching)
| Gesture | Action | How to Perform |
|---------|--------|----------------|
| ğŸ”„ Circle | **ORBIT** | Draw a circle with hand |
| ğŸ‘† Swipe Up | **FAST UP** | Quick upward motion |
| ğŸ‘‡ Swipe Down | **FAST DOWN** | Quick downward motion |
| ğŸ‘ˆ Swipe Left | **FAST LEFT** | Quick left motion |
| ğŸ‘‰ Swipe Right | **FAST RIGHT** | Quick right motion |
| ğŸ“¸ Fist â†’ Open | **PHOTO** | Close then open hand |
| ğŸ‘‹ Wave | **RETURN HOME** | Wave hand side to side |

### Keyboard Shortcuts
| Key | Action |
|-----|--------|
| `d` | Toggle between Static and Dynamic modes |
| `q` | Quit application |

## ğŸ“ Project Structure

```
gesture_drone_project/
â”œâ”€â”€ test.py                          # Main application (START HERE)
â”œâ”€â”€ test1.py
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ PROJECT_STRUCTURE.txt           # Detailed structure
â”‚
â”œâ”€â”€ models/                          # Trained ML models
â”‚   â”œâ”€â”€ gesture_model_knn.yml       # K-Nearest Neighbors model
â”‚   â”œâ”€â”€ gesture_model_cnn.h5        # Convolutional Neural Network
â”‚   â””â”€â”€ gesture_model_ann.pkl       # Artificial Neural Network
â”‚
â”œâ”€â”€ data/                            # Training and collected data
â”‚   â”œâ”€â”€ hand_images/                # CNN training images (by gesture)
â”‚   â”œâ”€â”€ training_data/              # KNN/ANN training data
â”‚   â””â”€â”€ sequences/                  # Dynamic gesture sequences
â”‚
â”œâ”€â”€ captured_images/                 # Photos taken with gestures
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ controllers/                # Drone controllers
â”‚   â”‚   â”œâ”€â”€ final_controller.py    # Production controller
â”‚   â”‚   â”œâ”€â”€ simple_controller.py   # Basic controller
â”‚   â”‚   â””â”€â”€ advanced_controller.py # Full-featured controller
â”‚   â”‚
â”‚   â”œâ”€â”€ data_collection/            # Data collection tools
â”‚   â”‚   â”œâ”€â”€ collect_images.py      # Collect hand images for CNN
â”‚   â”‚   â””â”€â”€ collect_static.py      # Collect landmark data for KNN/ANN
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                   # Model training scripts
â”‚   â”‚   â”œâ”€â”€ train_knn.py           # Train KNN model
â”‚   â”‚   â”œâ”€â”€ train_ann.py           # Train ANN model
â”‚   â”‚   â””â”€â”€ train_cnn.py           # Train CNN model
â”‚   â”‚
â”‚   â””â”€â”€ utils/                      # Utility modules
â”‚       â”œâ”€â”€ gesture_detection.py   # Gesture detection logic
â”‚       â”œâ”€â”€ ar_overlay.py          # AR visualization
â”‚       â””â”€â”€ online_learning.py     # Adaptive learning
â”‚
â””â”€â”€ config/
    â””â”€â”€ config.py                   # Configuration settings

```

## ğŸ› ï¸ How It Works

### 1. Hand Detection
Uses Google's **MediaPipe** to detect hands and extract 21 landmark points per hand in real-time.

### 2. Gesture Classification
Three ML models available (automatically selected):
- **KNN** - Fast, lightweight, good for real-time (Priority 1)
- **CNN** - Most accurate, best for static gestures (Priority 2)
- **ANN** - Balanced speed and accuracy (Priority 3)
- **Fallback** - Simple finger counting if no models found

### 3. Dynamic Gesture Detection
Analyzes hand motion over time (15-20 frames) to detect:
- Circular motions
- Directional swipes
- Hand opening/closing transitions
- Waving patterns

### 4. Mode System
**STATIC MODE** (Default):
- Only static gestures work
- Full drone control
- Can land normally

**DYNAMIC MODE** (Activated by ROCK gesture or 'd' key):
- Only dynamic gestures work
- Drone auto-hovers
- Must switch back to Static to land

This prevents confusion between static and dynamic gestures!

### 5. Drone Control
Currently uses a **MockDrone** for safe testing. Replace with real drone API (DJI Tello) for actual flight:
```python
# Replace MockDrone with:
from djitellopy import Tello
drone = Tello()
drone.connect()
```

## ğŸ“Š Training Your Own Models

### Step 1: Collect Data

**For CNN (Image-based):**
```bash
python src/data_collection/collect_images.py
```
- Press 0-9 to capture gestures
- Collect 50-100 images per gesture
- Images saved to `data/hand_images/`

**For KNN/ANN (Landmark-based):**
```bash
python src/data_collection/collect_static.py
```
- Press 0-9 to capture gestures
- Press 's' to save
- Data saved to `data/training_data/`

### Step 2: Train Models

**Train KNN:**
```bash
python src/training/train_knn.py
```

**Train CNN:**
```bash
python src/training/train_cnn.py
```

**Train ANN:**
```bash
python src/training/train_ann.py
```

Models are automatically saved to `models/` and will be loaded on next run.

## ğŸ¯ Performance

- **FPS**: 25-30 on average hardware
- **Latency**: 30-50ms gesture recognition
- **Accuracy**: 
  - KNN: 85-90%
  - CNN: 92-95%
  - ANN: 88-92%
  - Fallback: 70-80%

## ğŸ”§ Configuration

Edit `config/config.py` to customize:
```python
CAMERA_ID = 0                  # Camera index
CAMERA_WIDTH = 1280            # Resolution
CAMERA_HEIGHT = 720
MOVE_DISTANCE = 20             # Movement distance (cm)
CONFIDENCE_THRESHOLD = 0.75    # Minimum confidence
COOLDOWN_GESTURE = 0.5         # Gesture cooldown (seconds)
```

## ğŸ› Troubleshooting

### Camera Not Opening
```python
# Try different camera IDs in config/config.py
CAMERA_ID = 0  # Try 0, 1, 2, etc.
```

### Gestures Not Detected
1. Ensure good lighting
2. Hand fully visible to camera
3. Check confidence threshold
4. Train models with your own hand data

### Mode Not Switching
1. Verify ROCK gesture is correct:
   - Index finger: UP
   - Pinky finger: UP
   - Middle finger: DOWN
   - Ring finger: DOWN
2. Try 'd' key instead
3. Wait for cooldown (1.5 seconds)

### Low Accuracy
1. Collect more training data
2. Train with your specific hand
3. Adjust lighting conditions
4. Increase confidence threshold

## ğŸš€ Advanced Usage

### Using with Real Drone (DJI Tello)

1. Install DJI Tello SDK:
```bash
pip install djitellopy
```

2. Modify `test.py`:
```python
# Replace line ~40:
from djitellopy import Tello

# Replace MockDrone() with:
self.drone = Tello()
self.drone.connect()
```

3. **Safety First**:
- Test in open area
- Have manual override ready
- Start with low altitude
- Follow local regulations

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¨â€ğŸ’» Author

**Rishii Kumar Singh**
- GitHub: [@RishiiGamer2201](https://github.com/RishiiGamer2201)
- Project: [gesture_drone_project](https://github.com/RishiiGamer2201/gesture_drone_project)

## ğŸ“½ï¸ Video Demonstration
[Click here to watch the Project Demo Video](https://drive.google.com/file/d/1U7Jknz6b5JID3wG8v70Un_lgvapMVmsT/view?usp=sharing)

## ğŸ™ Acknowledgments

- **MediaPipe** by Google for hand tracking
- **OpenCV** for computer vision
- **TensorFlow** for deep learning
- **DJI Tello** for drone platform

## ğŸ“ Support

For issues, questions, or suggestions:
1. Open an issue on GitHub
2. Review existing issues for solutions

## ğŸ“ Learn More

This project demonstrates:
- Computer Vision with OpenCV
- Hand tracking with MediaPipe
- Machine Learning (KNN, CNN, ANN)
- Real-time gesture recognition
- State management
- AR visualization
- Drone control systems

Perfect for learning CV, ML, and robotics!

---

**âš ï¸ Safety Warning**: Always test with MockDrone first. Follow all safety guidelines and local regulations when using real drones.

**ğŸ¯ Ready to fly?** Run `python test.py` and start controlling with your hands!
