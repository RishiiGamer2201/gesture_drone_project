# ğŸš Advanced Gesture-Controlled Drone System

A complete gesture recognition system for drone control with machine learning, featuring dynamic gestures, hand pose estimation, AR overlay, and online learning.

## âœ¨ Features

### Core Features
- **Three ML Models**: KNN, ANN, and CNN implementations
- **Static Gestures**: 10 hand gestures for basic control (UP, DOWN, LEFT, RIGHT, FORWARD, BACKWARD, HOVER, LAND, FLIP, ROCK)
- **Dynamic Gestures**: CIRCLE, SWIPE (4 directions), OPEN/CLOSE, WAVE
- **Hand Pose Estimation**: 3D position and orientation tracking
- **Two-Hand Coordination**: Follow mode - drone follows your hand
- **AR Overlay**: Real-time visualization with trajectory and 3D position
- **Online Learning**: Adaptive model improvement during runtime
- **Mock Drone**: Safe testing without hardware

### Advanced Capabilities
- **Follow Mode**: Left fist + right open palm activates drone following
- **Image Capture**: Open/close gesture triggers camera
- **Gesture Sequences**: Combine gestures for complex commands
- **Confidence Scoring**: Visual feedback for gesture recognition quality
- **Adaptive Speed**: Hand distance controls movement speed
- **Rotation Control**: Hand tilt controls drone rotation

## ğŸ“ Project Structure

```
gesture_drone_project/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py                 # Central configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection/
â”‚   â”‚   â”œâ”€â”€ collect_static.py     # Collect static gesture data
â”‚   â”‚   â”œâ”€â”€ collect_dynamic.py    # Collect dynamic gesture sequences
â”‚   â”‚   â””â”€â”€ collect_images.py     # Collect images for CNN
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_knn.py          # Train KNN model
â”‚   â”‚   â”œâ”€â”€ train_ann.py          # Train ANN model
â”‚   â”‚   â””â”€â”€ train_cnn.py          # Train CNN model
â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â””â”€â”€ advanced_controller.py # Main advanced controller
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ gesture_detection.py   # Dynamic gesture detection
â”‚       â”œâ”€â”€ ar_overlay.py          # AR visualization
â”‚       â””â”€â”€ online_learning.py     # Adaptive learning
â”œâ”€â”€ models/                        # Trained models (generated)
â”œâ”€â”€ data/                          # Training data (generated)
â”œâ”€â”€ logs/                          # System logs
â””â”€â”€ README.md                      # This file
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt
```

### 2. Collect Training Data

```bash
# For KNN/ANN (landmarks)
python src/data_collection/collect_static.py

# For CNN (images)
python src/data_collection/collect_images.py

# For dynamic gestures
python src/data_collection/collect_dynamic.py
```

**Recommendation**: Collect 50-100 samples per gesture

### 3. Train Models

```bash
# Train KNN (fastest)
python src/training/train_knn.py

# Train ANN (best accuracy)
python src/training/train_ann.py

# Train CNN (most robust)
python src/training/train_cnn.py
```

### 4. Run Controller

```bash
# With CNN (recommended)
python src/controllers/advanced_controller.py --model cnn

# With ANN
python src/controllers/advanced_controller.py --model ann

# With KNN
python src/controllers/advanced_controller.py --model knn
```

## ğŸ® Gesture Controls

### Static Gestures (Single Frame)
| Gesture | Command | Description |
|---------|---------|-------------|
| ğŸ‘† Index Up | UP | Move drone up |
| ğŸ‘‡ Index Down | DOWN | Move drone down |
| ğŸ‘ˆ Index Left | LEFT | Move drone left |
| ğŸ‘‰ Index Right | RIGHT | Move drone right |
| ğŸ‘ Thumbs Up | FORWARD | Move forward |
| ğŸ‘ Thumbs Down | BACKWARD | Move backward |
| âœŒï¸ Peace Sign | FLIP | Backflip |
| âœŠ Fist | HOVER | Hover in place |
| âœ‹ Open Palm | LAND | Land drone |
| ğŸ¤˜ Rock | CHANGE MODE | Change mode from static to dyanamic and vice-verse |

### Dynamic Gestures (Motion-Based)
| Gesture | Command | Description |
|---------|---------|-------------|
| â­• Circle Motion | CIRCLE | Orbit mode |
| â† Swipe Left | SWIPE_LEFT | Fast left |
| â†’ Swipe Right | SWIPE_RIGHT | Fast right |
| â†‘ Swipe Up | SWIPE_UP | Fast up |
| â†“ Swipe Down | SWIPE_DOWN | Fast down |
| âœŠâ†’âœ‹ Fist to Open | OPEN_CLOSE | Capture image |
| ğŸ‘‹ Wave | WAVE | Return to home |

### Two-Hand Gestures
| Gesture | Command | Description |
|---------|---------|-------------|
| âœ‹âœ‹ Two Open Palms | TAKEOFF | Takeoff |
| âœŠâœŠ Two Fists | EMERGENCY | Emergency stop |
| âœŠâœ‹ Left Fist + Right Open | FOLLOW MODE | Drone follows hand |

## ğŸ¯ Model Comparison

| Feature | KNN | ANN | CNN |
|---------|-----|-----|-----|
| **Training Time** | <1s | 30-60s | 60-120s |
| **Prediction Speed** | 1ms | 5ms | 8ms |
| **Accuracy** | 85-95% | 90-98% | 92-99% |
| **Data Needed** | 20+ | 30+ | 50+ |
| **Best For** | Learning | Production | Research |

## ğŸ”§ Configuration

Edit `config/config.py` to customize:

- **Gesture sensitivity**
- **Movement distances**
- **Confidence thresholds**
- **AR overlay settings**
- **Online learning parameters**

## ğŸ“Š Advanced Features

### Hand Pose Estimation
- Estimates 3D hand position
- Tracks hand orientation (roll, pitch, yaw)
- Controls drone rotation with hand tilt
- Adjusts speed based on hand distance

### Follow Mode
- Activate with left fist + right open palm
- Drone maintains distance from target hand
- Auto-adjusts position as you move
- Deactivates automatically on other gestures

### Online Learning
- Corrects mispredictions in real-time
- Press 'c' to enter correction mode
- Model auto-updates after 50 corrections
- Personalizes to your gesture style

### AR Overlay
- Hand trajectory visualization
- 3D drone position display
- Confidence meter
- Gesture type indicators
- FPS counter

## ğŸ› Troubleshooting

**Camera not detected:**
```bash
# Test camera
python -c "import cv2; print('OK' if cv2.VideoCapture(0).read()[0] else 'FAIL')"
```

**Low accuracy:**
- Collect more diverse training data
- Ensure good lighting
- Make distinct gestures
- Use CNN model for best results

**Gestures not recognized:**
- Check confidence threshold in config
- Ensure entire hand is visible
- Improve lighting conditions
- Retrain with more samples

## ğŸ“ˆ Performance Tips

1. **Lighting**: Bright, even lighting works best
2. **Background**: Simple, uncluttered background
3. **Distance**: Keep hand 30-100cm from camera
4. **Gestures**: Make clear, exaggerated gestures
5. **Position**: Keep hand centered in frame

## ğŸ”¬ For Real Drone (DJI Tello)

1. Install djitellopy:
```bash
pip install djitellopy
```

2. Replace `AdvancedMockDrone` with:
```python
from djitellopy import Tello
self.drone = Tello()
```

3. Connect to Tello WiFi network

4. Test in safe, open area!

## ğŸ“ License

This project is for educational purposes.

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- More dynamic gestures
- Voice command integration
- Multi-drone control
- Mobile deployment
- Better visualization

## âš ï¸ Safety

**IMPORTANT** when using real drone:
- Always fly in open, safe areas
- Keep drone in sight
- Have manual override ready
- Follow local regulations
- Never fly near people/animals
- Test extensively in mock mode first

## Video Demonstration

[Click here to watch the Project Demo Video](https://drive.google.com/file/d/1U7Jknz6b5JID3wG8v70Un_lgvapMVmsT/view?usp=sharing)

---

**Ready to fly! ğŸšâœ¨**

*Built with â¤ï¸ using Python, OpenCV, MediaPipe, and TensorFlow*
