GESTURE DRONE PROJECT - COMPLETE FILE STRUCTURE
================================================

gesture_drone_project/
â”‚
â”œâ”€â”€ ðŸ“„ README.md                      # Main documentation
â”œâ”€â”€ ðŸ“„ SETUP_GUIDE.md                 # Step-by-step setup instructions
â”œâ”€â”€ ðŸ“„ requirements.txt               # Python dependencies
â”œâ”€â”€ ðŸš€ run.sh                         # Interactive launcher script
â”‚
â”œâ”€â”€ config/                           # Configuration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py                     # Central configuration file
â”‚
â”œâ”€â”€ src/                              # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_collection/              # Data collection tools
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ collect_static.py         # Collect static gestures
â”‚   â”‚   â”œâ”€â”€ collect_images.py         # Collect images for CNN
â”‚   â”‚   â””â”€â”€ collect_dynamic.py        # Collect dynamic sequences
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                     # Model training
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train_knn.py              # Train KNN model
â”‚   â”‚   â”œâ”€â”€ train_ann.py              # Train ANN model
â”‚   â”‚   â””â”€â”€ train_cnn.py              # Train CNN model
â”‚   â”‚
â”‚   â”œâ”€â”€ controllers/                  # Drone controllers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ final_controller.py    # Main advanced controller
â”‚   â”‚
â”‚   â””â”€â”€ utils/                        # Utility modules
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ gesture_detection.py      # Dynamic gesture detection
â”‚       â”œâ”€â”€ ar_overlay.py             # AR visualization
â”‚       â””â”€â”€ online_learning.py        # Adaptive learning
â”‚
â”œâ”€â”€ models/                           # Trained models (generated)
â”‚   # Will contain:
â”‚   # - gesture_model_knn.yml
â”‚   # - gesture_model_ann.h5
â”‚   # - gesture_model_cnn.h5
â”‚   # - gesture_scaler.pkl
â”‚   # - gesture_metadata_*.pkl
â”‚
â”œâ”€â”€ data/                             # Training data (generated)
â”‚   â”œâ”€â”€ training_data/                # Landmark data
â”‚   â”œâ”€â”€ hand_images/                  # Images for CNN
â”‚   â”œâ”€â”€ sequences/                    # Dynamic gesture sequences
â”‚   â””â”€â”€ online_learning/              # User corrections
â”‚
â”œâ”€â”€ logs/                             # System logs (generated)
â”‚
â”œâ”€â”€ test.py
â”œâ”€â”€ test1.py
â””â”€â”€ docs/                             # Additional documentation

FEATURES IMPLEMENTED:
====================

1. âœ… Dynamic Gestures
   - Circle detection (for orbit mode)
   - Swipe gestures (4 directions)
   - Open/Close (for image capture)
   - Wave detection

2. âœ… Hand Pose Estimation
   - 3D position tracking
   - Orientation estimation (roll, pitch, yaw)
   - Hand openness detection
   - Distance calculation

3. âœ… Two-Hand Coordination
   - Follow mode (left fist + right open)
   - Takeoff (two open palms)
   - Emergency (two fists)

4. âœ… Augmented Reality Overlay
   - Hand trajectory visualization
   - 3D drone position display
   - Confidence meter
   - Dynamic gesture trails
   - FPS counter

5. âœ… Online/Adaptive Learning
   - Real-time correction interface
   - Model fine-tuning
   - Correction statistics
   - Auto-save and retrain

MODELS INCLUDED:
===============

1. KNN (K-Nearest Neighbors)
   - Fastest training (<1 second)
   - Simple and interpretable
   - 85-95% accuracy

2. ANN (Artificial Neural Network)
   - 3-layer fully connected network
   - Batch normalization
   - 90-98% accuracy

3. CNN (Convolutional Neural Network)
   - Image-based recognition
   - 3 convolutional blocks
   - 92-99% accuracy

GESTURES SUPPORTED:
==================

Static (9 gestures):
- UP, DOWN, LEFT, RIGHT
- FORWARD, BACKWARD
- HOVER, LAND, FLIP

Dynamic (7 gestures):
- CIRCLE
- SWIPE_LEFT, SWIPE_RIGHT, SWIPE_UP, SWIPE_DOWN
- OPEN_CLOSE
- WAVE

Two-Hand (3 gestures):
- TAKEOFF (two palms)
- EMERGENCY (two fists)
- FOLLOW_MODE (left fist + right open)

TOTAL: 19 gesture commands!

HOW TO USE:
==========

1. Run ./run.sh for interactive menu
2. Or use direct commands:
   - python3 src/data_collection/collect_static.py
   - python3 src/training/train_knn.py
   - python3 src/controllers/advanced_controller.py --model knn

REQUIREMENTS:
============

- Python 3.8+
- Webcam
- 2GB free space
- opencv-python, mediapipe, tensorflow, scikit-learn

